{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUZZyOJcl1n7Wez1UEUyKx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alton1998/LLM-Detect-AI-Generated-Text/blob/main/LLM_LLM_Generated_Text_Detection_Pretrained_MultinomialNaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm6p-rgmdA0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import string\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "id": "on8G4mWqdt3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "1pbtj961eJ61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_fake_text_data_set_train = load_dataset(\"yaful/DeepfakeTextDetect\", split=\"train\")\n",
        "deep_fake_text_data_set_val = load_dataset(\"yaful/DeepfakeTextDetect\", split=\"validation\")\n",
        "deep_fake_text_data_set_test = load_dataset(\"yaful/DeepfakeTextDetect\", split=\"test\")"
      ],
      "metadata": {
        "id": "27UZKXB1eAT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_fake_text_data_set_train_df = deep_fake_text_data_set_train.to_pandas()\n",
        "deep_fake_text_data_set_val_df = deep_fake_text_data_set_val.to_pandas()\n",
        "deep_fake_text_data_set_test_df = deep_fake_text_data_set_test.to_pandas()\n",
        "\n",
        "def invert_label(x):\n",
        "  if x==1:\n",
        "    return 0\n",
        "  elif x==0:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "3tV5UBfTeopZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_fake_text_data_set_train_df.shape"
      ],
      "metadata": {
        "id": "jfwBhWCKBKmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_fake_text_data_set_train_df[\"new label\"] = deep_fake_text_data_set_train_df['label'].apply(invert_label)\n",
        "deep_fake_text_data_set_val_df[\"new label\"] =  deep_fake_text_data_set_val_df['label'].apply(invert_label)\n",
        "deep_fake_text_data_set_test_df[\"new label\"] = deep_fake_text_data_set_test_df['label'].apply(invert_label)"
      ],
      "metadata": {
        "id": "BJgudHP5p5Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_fake_text_data_set_train_df['length'] = deep_fake_text_data_set_train_df['text'].apply(len)\n",
        "deep_fake_text_data_set_val_df['length'] = deep_fake_text_data_set_val_df['text'].apply(len)\n",
        "deep_fake_text_data_set_test_df['length'] = deep_fake_text_data_set_test_df['text'].apply(len)"
      ],
      "metadata": {
        "id": "2aSnbxvCDgdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_process(mess):\n",
        "    \"\"\"\n",
        "    1.remove punc\n",
        "    2. remove stop words\n",
        "    3. return list of clean text words\n",
        "    \"\"\"\n",
        "    nopunc = [char for char in mess if char not in string.punctuation]\n",
        "    nopunc = \"\".join(nopunc)\n",
        "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "RzZC496nIIQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train,text_test,label_train,label_test = train_test_split(deep_fake_text_data_set_train_df['text'],deep_fake_text_data_set_train_df['new label'],test_size=0.3,random_state=101)"
      ],
      "metadata": {
        "id": "gBm6MnFZsom0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "UCG2z0pOpba9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('bow',CountVectorizer(analyzer=text_process)),\n",
        "    ('tfidf',TfidfTransformer()),\n",
        "    ('classifier',MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "-AMd6eoUttUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "pipeline.fit(text_train,label_train)\n",
        "print(f'\\nDuration: {time.time() - start_time:.0f} seconds')"
      ],
      "metadata": {
        "id": "kTyHMKEkt1Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = pipeline.predict(text_test)"
      ],
      "metadata": {
        "id": "LGdExdjEr8UV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(label_test,predictions))"
      ],
      "metadata": {
        "id": "GiH57QjNuDpy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}